{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snakemake configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short ping pong with the Asko sample processing between UPPMAX and my personal folder I decided to add Snakemake in the mix. It is using the same configure.yaml and it can be used from pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### conda\n",
    "\n",
    "The pipeline fails on UPPMAX when using modules loaded outside conda. \n",
    "\n",
    "TODO: Cluster running the main snakefile doesn't seem to be the main design. It seems that the best approach is to simply configure temporary batch jobs for the cluster designed rules and keep track of them. But how can I take the *shell:* section specified rules and dump them in a file? And how to lock against incidentally re-running the same jobs before they get to end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
